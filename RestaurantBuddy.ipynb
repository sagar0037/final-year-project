{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 patterns.\n",
      "There are 6 tags which are: ['cleanliness', 'drink', 'greet', 'offer', 'often', 'popular']\n",
      "There are 36 uniquly stemmed sorted words which are: ['alcohol', 'ani', 'anyon', 'be', 'can', 'cleanli', 'do', 'drink', 'for', 'have', 'hello', 'hey', 'hi', 'how', 'i', 'is', 'it', 'maintain', 'mainten', 'meal', 'of', 'offer', 'often', 'oftenli', 'popular', 'restaur', 'sanit', 'serv', 'someth', 'the', 'there', 'to', 'want', 'what', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()#stemming\n",
    "\n",
    "#creating the custom functions\n",
    "\n",
    "# tokenize each word in the sentence\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "# stem and lower the word\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "splitting each word in the sentences and adding it to an array\n",
    "If words exists in the sentence, it returns 1 else 0\n",
    "example:\n",
    "sentence = [\"how\", \"are\", \"you\", \"today\"]\n",
    "words = [\"hi\", \"hello\", \"how\", \"bye\", \"today\", \"welcome\"]\n",
    "It provides:\n",
    "bag   = [  0 ,    0   ,   1  ,   0  ,    1   ,     0    ]\n",
    "\"\"\"\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for index, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[index] = 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "#loading the restaurants.json file\n",
    "with open('restaurants.json', 'r') as f:\n",
    "    restaurants = json.load(f)\n",
    "\n",
    "allWords = []\n",
    "tags = []\n",
    "xy_data = []\n",
    "\n",
    "# looping through every sentence in restaurant patterns\n",
    "for restaurant in restaurants['restaurants']:\n",
    "    tag = restaurant['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in restaurant['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)# adding to tag list\n",
    "        allWords.extend(w)\n",
    "        xy_data.append((w, tag))\n",
    "\n",
    "#cleaning the data\n",
    "ignore_words = ['.', '!', '?']\n",
    "allWords = [stem(w) for w in allWords if w not in ignore_words]\n",
    "\n",
    "# remove duplicate data and sorting\n",
    "allWords = sorted(set(allWords))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(\"There are\", len(xy_data), \"patterns.\")\n",
    "print(\"There are\", len(tags), \"tags which are:\", tags)\n",
    "print(\"There are\", len(allWords), \"uniquly stemmed sorted words which are:\", allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size  : 36\n",
      "Output size : 6\n",
      "Epoch [100/1000], Loss: 1.2163\n",
      "Epoch [200/1000], Loss: 0.6545\n",
      "Epoch [300/1000], Loss: 0.0791\n",
      "Epoch [400/1000], Loss: 0.0110\n",
      "Epoch [500/1000], Loss: 0.0068\n",
      "Epoch [600/1000], Loss: 0.0016\n",
      "Epoch [700/1000], Loss: 0.0025\n",
      "Epoch [800/1000], Loss: 0.0023\n",
      "Epoch [900/1000], Loss: 0.0017\n",
      "Epoch [1000/1000], Loss: 0.0009\n",
      "final loss: 0.0009\n",
      "Training complete. File has been saved to filedata.pth\n"
     ]
    }
   ],
   "source": [
    "# create the training data\n",
    "train_x = []\n",
    "train_y = []\n",
    "for (pattern_sentence, tag) in xy_data:\n",
    "    # x: bag of words for each pattern_info\n",
    "    bag = bag_of_words(pattern_sentence, allWords)\n",
    "    train_x.append(bag)\n",
    "    \n",
    "    # y: PyTorch CrossEntropyLoss only needs class labels\n",
    "    label = tags.index(tag)\n",
    "    train_y.append(label)\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, noClasses):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.n1 = nn.Linear(inputSize, hiddenSize) \n",
    "        self.n2 = nn.Linear(hiddenSize, hiddenSize) \n",
    "        self.n3 = nn.Linear(hiddenSize, noClasses)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.n1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.n2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.n3(out)\n",
    "        return out\n",
    "    \n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(train_x)\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Hyper-parameters for Neural network\n",
    "numEpoches = 1000\n",
    "batch_size = 8\n",
    "learningRate = 0.001\n",
    "inputSize = len(train_x[0])\n",
    "hiddenSize = 8\n",
    "outputSize = len(tags)\n",
    "print(\"Input size  :\", inputSize)\n",
    "print(\"Output size :\", outputSize)\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNetwork(inputSize, hiddenSize, outputSize).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(numEpoches):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{numEpoches}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"modelState\": model.state_dict(),\n",
    "\"inputSize\": inputSize,\n",
    "\"hiddenSize\": hiddenSize,\n",
    "\"outputSize\": outputSize,\n",
    "\"allWords\": allWords,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "#saving the trained model\n",
    "filename = \"filedata.pth\"\n",
    "torch.save(data, filename)\n",
    "\n",
    "print(f'Training complete. File has been saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (n1): Linear(in_features=36, out_features=8, bias=True)\n",
       "  (n2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (n3): Linear(in_features=8, out_features=6, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the saved model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('restaurants.json', 'r') as json_data:\n",
    "    restaurants = json.load(json_data)\n",
    "\n",
    "filename = \"filedata.pth\"\n",
    "data = torch.load(filename)\n",
    "\n",
    "inputSize = data[\"inputSize\"]\n",
    "hiddenSize = data[\"hiddenSize\"]\n",
    "outputSize = data[\"outputSize\"]\n",
    "allWords = data['allWords']\n",
    "tags = data['tags']\n",
    "modelState = data[\"modelState\"]\n",
    "\n",
    "model = NeuralNetwork(inputSize, hiddenSize, outputSize).to(device)\n",
    "model.load_state_dict(modelState)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the restaurant. I am your restaurant buddy.\n",
      "You can enter exit to finish the conversation anytime.\n",
      "You  : Hi\n",
      "Buddy: What can I do for you?\n",
      "You  : Hey\n",
      "Buddy: How can I help\n",
      "You  : I want something to drink\n",
      "Buddy: You can have wine or beer\n",
      "You  : What is restaurant popular for?\n",
      "Buddy: It is popular for Chicken Biryani\n",
      "You  : Any offer to me as dish\n",
      "Buddy: We provide you wine and beer\n",
      "You  : exit\n"
     ]
    }
   ],
   "source": [
    "#use of the chatbot\n",
    "botname = \"Buddy\"\n",
    "print(\"Welcome to the restaurant. I am your restaurant buddy.\")\n",
    "print(\"You can enter exit to finish the conversation anytime.\")\n",
    "while True:\n",
    "    sentence = input(\"You  : \")\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, allWords)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for restaurant in restaurants['restaurants']:\n",
    "            if tag == restaurant[\"tag\"]:\n",
    "                print(f\"{botname}: {random.choice(restaurant['replys'])}\")\n",
    "    else:\n",
    "        print(f\"{botname}: I do not understand what you want to say.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
